{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1573b14",
   "metadata": {},
   "source": [
    "# PCA Analysis\n",
    "\n",
    "We have hourly count data, over a lot of years and locations.\n",
    "\n",
    "**Can we simplify this data?**\n",
    "\n",
    "Do we really need 24 numbers to describe a days worth of data in a single location? Because we have such regularity in the data (daily, weekly cycle), I suspect that we don't. PCA would be a good place to start investigating this.\n",
    "\n",
    "**Visualisation**\n",
    "\n",
    "By visualising the transformed space, what insights can we get into urban geography.\n",
    "\n",
    "\n",
    "**Denoising**\n",
    "\n",
    "I think building forecasters based on the hourly data will be an intense process. If we could reduce a days/weeks worth of data down to a few numbers this could help with that.\n",
    "\n",
    "**Clustering?**\n",
    "\n",
    "PCA can be used as step in a clustering pipeline. This is typically used for denoising purposes (often seen in analysis of MNIST or Hand Written Digit data sets). So it might feed in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4e161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "from IPython.core.display import Javascript\n",
    "from IPython.display import HTML\n",
    "\n",
    "from PedestrianDataImporter import getHourlyCounts\n",
    "from Imputation import imputeMissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c981bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also disable the scroll. I just don't like it\n",
    "def disable_scroll():\n",
    "    display(Javascript(\"\"\"\n",
    "        IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "            return false;\n",
    "        }\n",
    "    \"\"\"))\n",
    "\n",
    "disable_scroll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d04c2d",
   "metadata": {},
   "source": [
    "## Data importing and some manipulations\n",
    "\n",
    "Lets first import the Melbourne data and cut out a time window in 2018 and 2019 to deal with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd461495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts,df_locations = getHourlyCounts['Melbourne']()\n",
    "df_counts = imputeMissing(df_counts)\n",
    "\n",
    "start_date = datetime.datetime(2018,1,1)\n",
    "end_date   = datetime.datetime(2020,1,1)\n",
    "cut = (df_counts['DateTime'] >  start_date) & (df_counts['DateTime'] < end_date ) \n",
    "df_cut = df_counts.loc[cut]\n",
    "df1 = df_cut[['DateTime','LocationName','HourlyCount']]\n",
    "df1 = df1.dropna(axis='columns',how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70edd6",
   "metadata": {},
   "source": [
    "We want vectors that represent a days worth of counts or a weeks worth of counts.\n",
    "While we at it lets also take just weekdays and weekends, we have seen that these have different behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d455e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Day']  = df1['DateTime'].dt.dayofweek  # 0: Monday, 1: Tuesday, ..., 6: Sunday\n",
    "df1['Hour'] = df1['DateTime'].dt.hour\n",
    "df1['Week'] = df1['DateTime'].dt.isocalendar().week\n",
    "df1['Year'] = df1['DateTime'].dt.year\n",
    "df1['Date'] = df1['DateTime'].dt.date\n",
    "\n",
    "df1['Day_Hour'] = df1['Day'] * 24 + df1['Hour']\n",
    "df1['Year_Week'] = df1['Year'].astype(str) + \"_\" + df1['Week'].astype(str).str.zfill(2)  # zfill ensures week 1 is \"01\", week 2 is \"02\", etc.\n",
    "\n",
    "weekdays = df1['Day'] < 5\n",
    "weekends = df1['Day'] >= 5\n",
    "\n",
    "df_days = df1.pivot_table(index=['LocationName', 'Date'],\n",
    "    columns='Hour', values='HourlyCount', aggfunc='sum').reset_index()\n",
    "\n",
    "df_weekdays = df1.loc[weekdays].pivot_table(index=['LocationName', 'Date'],\n",
    "    columns='Hour', values='HourlyCount', aggfunc='sum').reset_index()\n",
    "\n",
    "df_weekends = df1.loc[weekends].pivot_table(index=['LocationName', 'Date'],\n",
    "    columns='Hour', values='HourlyCount', aggfunc='sum').reset_index()\n",
    "\n",
    "df_weeks = df1.pivot_table(index=['LocationName', 'Year_Week'], \n",
    "    columns='Day_Hour', values='HourlyCount', aggfunc='sum').reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921526a",
   "metadata": {},
   "source": [
    "Now we have datatables with the aggregated days and weeks, we want some numpy arrays.\n",
    "We will also throw out any rows with missing data. This isn't perfect but it will do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef573598",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursdays_columns = list(range(24))  # Assuming hours are from 0 to 23 after the pivot\n",
    "hoursweeks_columns = list(range(7*24))  # Assuming hours are from 0 to 23 after the pivot\n",
    "\n",
    "def aggregatedDFtoArray(df, columns):\n",
    "    data = df[columns]\n",
    "    arr = data.values\n",
    "    mask = ~np.isnan(arr).any(axis=1)  # Create a mask for rows without NaN values\n",
    "    arr = arr[mask]  # Filter out rows with NaN values in arr\n",
    "    location_names = df['LocationName'].values[mask]  # Apply the same mask to the location names\n",
    "    return arr, location_names\n",
    "\n",
    "\n",
    "day_array,day_labels         = aggregatedDFtoArray(df_days ,hoursdays_columns)\n",
    "weekday_array,weekday_labels = aggregatedDFtoArray(df_weekdays,hoursdays_columns)\n",
    "weekend_array,weekend_labels = aggregatedDFtoArray(df_weekends,hoursdays_columns)\n",
    "week_array,week_labels        = aggregatedDFtoArray(df_weeks,hoursweeks_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78881594",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Lets plot the average for the days/weeks (in <span style=\"color:red\">red</span> ), plot some samples in <span style=\"color:blue\">blue</span>. A little transparency help us visualise the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f60a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "N = 100\n",
    "\n",
    "def plotAggregatedArray(arr,N,alpha,title,ax):\n",
    "    # Shuffle them to mix up the locations and times.\n",
    "    total_rows = len(arr)\n",
    "    inds = np.random.choice(total_rows, N, replace=False)\n",
    "    ax.plot(arr[inds,:].T,'b-',alpha=alpha)\n",
    "    ax.plot(np.mean(arr,axis=0),'r')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    \n",
    "fig,ax = plt.subplots(2,1)\n",
    "plotAggregatedArray(day_array, N, alpha, 'Days',ax[0])\n",
    "ax[0].set_xticks(np.arange(0,26,2))\n",
    "plotAggregatedArray(week_array, N, alpha, 'Weeks',ax[1])\n",
    "ax[1].set_xticks(np.arange(0,24*7 + 2,24))\n",
    "plt.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(2,1)\n",
    "plotAggregatedArray(weekday_array, N, alpha, 'Weekdays',ax[0])\n",
    "ax[0].set_xticks(np.arange(0,26,2))\n",
    "plotAggregatedArray(weekend_array, N, alpha, 'Weekends',ax[1])\n",
    "ax[1].set_xticks(np.arange(0,26,2))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8666370",
   "metadata": {},
   "source": [
    "### What do we see "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cfd2a",
   "metadata": {},
   "source": [
    "Looking at\n",
    "* **Week days** we see peaks around 8AM, 12PM and 17PM. Corresponding to commutes to work, lunch, and commute home from work\n",
    "* **Week ends** we don't see the three peaks that we see during the weekdays. Slightly busier during the late hours 12PM-2AM corresponding to visits to a night life.\n",
    "* **Weeks** we clearly see the weekday behaviour and weekend behaviour. Visually I can't see any different between the weekdays.\n",
    "* **Days**  The weekday behaviour dominates but has been damped down by the inclusion of the weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c9930",
   "metadata": {},
   "source": [
    "## Choice of pipeline\n",
    "\n",
    "Now we have a better understanding of the data. Lets start playing around with a PCA analysis.\n",
    "\n",
    "I want to think about the effect of scaling the data on the principal components. A variance scaling step can be used as a preprocessing step in a PCA pipeline. This is most useful when the data has different *units*, for example if we measured the heights of people in nanometres and their weights in solar masses, the height dimension would show much greater variation and we would want to remove/scale this variation.\n",
    "\n",
    "In our problem the features are the different hours of the day and are measured in the same units (number of people), scaling each hour to have the same variance would put equal importance on each of the day. It would make us *explain* the details of the low count hours (during the night) as much as the high count hours (during the day). I don't think this is what we want to do, but lets investigate it anyway.\n",
    "\n",
    "The PCA routine in sklearn removes the mean anyway (as it should)\n",
    "So lets consider two pipelines\n",
    "\n",
    "* Pipeline1: Remove the mean (can't hurt to do it twice), but DON'T scale the variables.\n",
    "* Pipeline2: Remove the mean but DO scale the variables. i.e. the variance for 0AM will be as large as for 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d490ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([\n",
    "    ('scaler', StandardScaler(\n",
    "                                with_mean = True,\n",
    "                                with_std = False,\n",
    "                             )),\n",
    "    ('pca', PCA())  \n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('scaler', StandardScaler(\n",
    "                                with_mean = True,\n",
    "                                with_std = True,\n",
    "                             )),\n",
    "    ('pca', PCA()) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2b4a2",
   "metadata": {},
   "source": [
    "Lets group all the pipelines and data together into dictionaries and run out pipelines over them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df04d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "             'Raw':pipeline1,\n",
    "             'Scaled Var':pipeline2,             \n",
    "            }\n",
    "\n",
    "datasets = {\n",
    "             'Day':day_array,\n",
    "             'Weekday':weekday_array,\n",
    "             'Weekend':weekend_array,\n",
    "             'Week':week_array,\n",
    "           }\n",
    "\n",
    "# Fit all these combinations of pipelines and data.\n",
    "# What can I say, I love a massive nested dictionary comphrension.\n",
    "pipelines_fitted = { dataset_name:{pipeline_name:clone(pipeline).fit(data)\n",
    "                         for pipeline_name,pipeline in pipelines.items()}\n",
    "                                  for dataset_name,data in datasets.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639df7ba",
   "metadata": {},
   "source": [
    "Lets look at the explained variance and cumulative explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ea8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_from_pipeline(pipeline):\n",
    "    for step in pipeline.steps:\n",
    "        if isinstance(step[1], PCA):\n",
    "            return step[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66ee7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotExplainedVariances(fitted_pipelines,axs):\n",
    "    marker_cycle = itertools.cycle(('s', 'o', '^', '*')) \n",
    "    for pipeline_name,pipeline in fitted_pipelines.items():\n",
    "        pca = get_pca_from_pipeline(pipeline)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "        marker = next(marker_cycle)\n",
    "        \n",
    "        axs[0].plot(np.arange(1,len(explained_variance)+1),explained_variance,marker=marker)\n",
    "        axs[1].plot(np.arange(1,len(cumulative_variance)+1),cumulative_variance,marker=marker)\n",
    "        \n",
    "    axs[0].set_title('Explained Variance')\n",
    "    axs[1].set_title('Cumulative Explained Variance')\n",
    "    for ax in axs:\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.legend(fitted_pipelines.keys())\n",
    "        ax.set_xlim([0,20])\n",
    "        ax.set_xticks(np.arange(20))\n",
    "        ax.set_xlabel('Number of components')\n",
    "        ax.grid()\n",
    "\n",
    "        \n",
    "fig,axs = plt.subplots(len(datasets),2,figsize=(8,12))\n",
    "i=0\n",
    "for dataset_name,ps in pipelines_fitted.items():\n",
    "    plotExplainedVariances(ps, axs[i,:])\n",
    "    axs[i,0].set_title(axs[i,0].get_title() + f\"\\n{dataset_name}\")\n",
    "    axs[i,1].set_title(axs[i,1].get_title() + f\"\\n{dataset_name}\")\n",
    "    i+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc78c60",
   "metadata": {},
   "source": [
    "What do we see here?\n",
    "* Most of the variance in the data is explained in the first principal component, after around 3 components we are already explaining about 90% of the variance of the data!\n",
    "* Scaling the variance makes everything much harder. We have to put more effort into explaining the details of the early hours and need more principal components to explain the data.\n",
    "\n",
    "Lets ignore the scaled variance approach then.\n",
    "\n",
    "Lets compare these different datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153895bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1)\n",
    "marker_cycle = itertools.cycle(('s', 'o', '^', '*')) \n",
    "for dataset_name,ps in pipelines_fitted.items():\n",
    "    pca = get_pca_from_pipeline(ps['Raw'])\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    marker = next(marker_cycle)\n",
    "    axs.plot(np.arange(1,len(cumulative_variance)+1),cumulative_variance,marker=marker)\n",
    "    \n",
    "axs.set_ylim(0,1)\n",
    "axs.legend(datasets.keys())\n",
    "axs.set_xlim([0,20])\n",
    "axs.set_xticks(np.arange(20))\n",
    "axs.set_xlabel('Number of components')\n",
    "axs.set_ylabel('Explained Variance')\n",
    "axs.grid()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fd834",
   "metadata": {},
   "source": [
    "What do we see here. We can think about this in terms of *ease of explanations*. \n",
    "\n",
    "How many components do we need to explain the data. In order of easyness (easiest to hardest).\n",
    "\n",
    "1. Weekdays\n",
    "2. Days.\n",
    "3. Weekends.\n",
    "4. Weeks\n",
    "\n",
    "This means that with (for example) 3 components. We explain more the variance, over all the locations, of weekday behaviour than daily behaviour. This makes sense, the day behaviour contains both week and weekday behaviour. Weeks the most difficult to explain (lower explained variance with 3 components) again this makes sense, there is more data there and more combinations of weekday and weekend behaviour are possible.\n",
    "\n",
    "WHY ARE WEEKENDS MORE DIFFICULT TO EXPLAIN THAN DAYS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6372b7",
   "metadata": {},
   "source": [
    "## Inspecting the projections\n",
    "\n",
    "What does this all mean?\n",
    "Lets take our daily data and inspect the principal components.\n",
    "When we reconstruct from our transformed components, remember we add back the mean so lets look at that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e6ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = day_array\n",
    "labels = day_labels\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('pca', PCA())  \n",
    "])\n",
    "\n",
    "fitted = pipeline1.fit_transform(arr)\n",
    "pca = get_pca_from_pipeline(pipeline1)\n",
    "comps = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def27b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkYlims(axs):\n",
    "    ymin = min(ax.get_ylim()[0] for ax in axs) \n",
    "    ymax = max(ax.get_ylim()[1] for ax in axs) \n",
    "    for ax in axs:\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.mean(arr,axis=0))\n",
    "ax.set_title('Mean')\n",
    "ax.set_xticks(range(0, 24, 6))\n",
    "ax.set_xticks(range(0, 24), minor=True)\n",
    "ax.grid(which='major', axis='x', linestyle='-')\n",
    "        \n",
    "fig,axs = plt.subplots(2,3)\n",
    "axs = axs.flatten()\n",
    "for ax,row,c in zip(axs,comps,itertools.count(1)):\n",
    "    ax.plot(row)\n",
    "    ax.set_title(f\"Component {c}\")\n",
    "    ax.set_xticks(range(0, 24, 6))\n",
    "    ax.set_xticks(range(0, 24), minor=True)\n",
    "    ax.grid(which='major', axis='x', linestyle='-')\n",
    "\n",
    "linkYlims(axs)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d32f08",
   "metadata": {},
   "source": [
    "This is quite interesting. I see some patterns in this.\n",
    "* **Component 1**: This looks a lot like the mean. Adding or subtracting this would account for general *busyness* during the day.\n",
    "* **Component 2**: This exaggerates the early morning and late afternoon peaks.\n",
    "* **Component 3**: Decreases the midday counts.\n",
    "* **Component 4**: Decreases the late afternoon peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390251b",
   "metadata": {},
   "source": [
    "Lets take a look at some of the daily counts what components they have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e56c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(fitted)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "inds = rng.choice(total_rows, N, replace=False)\n",
    "\n",
    "fig,axs = plt.subplots(2,3,figsize=(12,6))\n",
    "axs = axs.flatten()\n",
    "for ax,row,c in zip(axs,fitted[inds,:],itertools.count()):\n",
    "    ax.stem(row)\n",
    "    ax.set_title(f\"Components Example {c}\")\n",
    "linkYlims(axs)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig,axs = plt.subplots(2,3,figsize=(12,6))\n",
    "axs = axs.flatten()\n",
    "for ax,row in zip(axs,arr[inds,:]):\n",
    "    ax.plot(row)\n",
    "    ax.set_title(f\"Time series {c}\")\n",
    "linkYlims(axs)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c34517",
   "metadata": {},
   "source": [
    "## Plotting the components in the transformed space\n",
    "\n",
    "The hope here is that we might be able to visualise clusters in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c7eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "comps_plots = [1,2,3]\n",
    "trace = go.Scatter3d(\n",
    "        x=fitted[::10, comps_plots[0]-1],\n",
    "        y=fitted[::10, comps_plots[1]-1],\n",
    "        z=fitted[::10, comps_plots[2]-1],\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='circle', size=1),\n",
    "        #name=str(label)\n",
    "    )\n",
    "traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    "    scene=dict(\n",
    "        xaxis_title=f\"comp{comps_plots[0]}\",\n",
    "        yaxis_title=f\"comp{comps_plots[1]}\",\n",
    "        zaxis_title=f\"comp{comps_plots[2]}\"\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,  # Adjust as needed\n",
    "    height=800   # Adjust as needed\n",
    ")\n",
    "\n",
    "# Create the figure and display it\n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04873ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_markers = ['circle', 'square', 'diamond']\n",
    "\n",
    "comps_plots = [1, 2, 3]\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Create a scatter plot for each label\n",
    "traces = []\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    data_for_label = fitted[labels == label]\n",
    "    trace = go.Scatter3d(\n",
    "        x=data_for_label[::1, comps_plots[0]-1],\n",
    "        y=data_for_label[::1, comps_plots[1]-1],\n",
    "        z=data_for_label[::1, comps_plots[2]-1],\n",
    "        mode='markers',\n",
    "        marker=dict(symbol=plotly_markers[idx % len(plotly_markers)], size=2),\n",
    "        name=str(label)\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=f\"comp{comps_plots[0]}\",\n",
    "        yaxis_title=f\"comp{comps_plots[1]}\",\n",
    "        zaxis_title=f\"comp{comps_plots[2]}\"\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,  # Adjust as needed\n",
    "    height=800   # Adjust as needed\n",
    ")\n",
    "\n",
    "# Create the figure and display it\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50086a28",
   "metadata": {},
   "source": [
    "## What about Weekly behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbd75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = week_array\n",
    "labels = week_labels\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('pca', PCA())  \n",
    "])\n",
    "\n",
    "fitted = pipeline1.fit_transform(arr)\n",
    "pca = get_pca_from_pipeline(pipeline1)\n",
    "comps = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "345e45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkYlims(axs):\n",
    "    ymin = min(ax.get_ylim()[0] for ax in axs) \n",
    "    ymax = max(ax.get_ylim()[1] for ax in axs) \n",
    "    for ax in axs:\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.mean(arr,axis=0))\n",
    "ax.set_title('Mean')\n",
    "ax.set_xticks(range(0, (24+1)*7, 24))\n",
    "ax.set_xticks(range(0, (24+1)*7, 24), minor=True)\n",
    "ax.grid(which='major', axis='x', linestyle='-')\n",
    "        \n",
    "fig,axs = plt.subplots(2,3)\n",
    "axs = axs.flatten()\n",
    "for ax,row,c in zip(axs,comps,itertools.count(1)):\n",
    "    ax.plot(row)\n",
    "    ax.set_title(f\"Component {c}\")\n",
    "    ax.set_xticks(range(0, (24+1)*7, 24))\n",
    "    ax.set_xticks(range(0, (24+1)*7, 24), minor=True)\n",
    "    ax.grid(which='major', axis='x', linestyle='-')\n",
    "\n",
    "linkYlims(axs)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c29d2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(fitted)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "inds = rng.choice(total_rows, N, replace=False)\n",
    "\n",
    "fig,axs = plt.subplots(2,3)\n",
    "axs = axs.flatten()\n",
    "for ax,row,c in zip(axs,fitted[inds,:],itertools.count()):\n",
    "    ax.stem(row)\n",
    "    ax.set_title(f\"Components Example {c}\")\n",
    "linkYlims(axs)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig,axs = plt.subplots(2,3)\n",
    "axs = axs.flatten()\n",
    "for ax,row in zip(axs,arr[inds,:]):\n",
    "    ax.plot(row)\n",
    "    ax.set_title(f\"Time series {c}\")\n",
    "linkYlims(axs)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2678fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "comps_plots = [1,2,3]\n",
    "trace = go.Scatter3d(\n",
    "        x=fitted[::1, comps_plots[0]-1],\n",
    "        y=fitted[::1, comps_plots[1]-1],\n",
    "        z=fitted[::1, comps_plots[2]-1],\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='circle', size=1),\n",
    "        #name=str(label)\n",
    "    )\n",
    "traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    "    scene=dict(\n",
    "        xaxis_title=f\"comp{comps_plots[0]}\",\n",
    "        yaxis_title=f\"comp{comps_plots[1]}\",\n",
    "        zaxis_title=f\"comp{comps_plots[2]}\"\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,  # Adjust as needed\n",
    "    height=800   # Adjust as needed\n",
    ")\n",
    "\n",
    "# Create the figure and display it\n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1109fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_markers = ['circle', 'square', 'diamond']\n",
    "\n",
    "comps_plots = [1, 2, 3]\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Create a scatter plot for each label\n",
    "traces = []\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    data_for_label = fitted[labels == label]\n",
    "    trace = go.Scatter3d(\n",
    "        x=data_for_label[::1, comps_plots[0]-1],\n",
    "        y=data_for_label[::1, comps_plots[1]-1],\n",
    "        z=data_for_label[::1, comps_plots[2]-1],\n",
    "        mode='markers',\n",
    "        marker=dict(symbol=plotly_markers[idx % len(plotly_markers)], size=2),\n",
    "        name=str(label)\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=f\"comp{comps_plots[0]}\",\n",
    "        yaxis_title=f\"comp{comps_plots[1]}\",\n",
    "        zaxis_title=f\"comp{comps_plots[2]}\"\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,  # Adjust as needed\n",
    "    height=800   # Adjust as needed\n",
    ")\n",
    "\n",
    "# Create the figure and display it\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c37398",
   "metadata": {},
   "source": [
    "## Pause for thought\n",
    "\n",
    "Lets plot the first PCA components for the daily data and have a close inspection of the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "744123a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = day_array\n",
    "labels = day_labels\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('pca', PCA())  \n",
    "])\n",
    "\n",
    "fitted = pipeline1.fit_transform(arr)\n",
    "pca = get_pca_from_pipeline(pipeline1)\n",
    "comps = pca.components_\n",
    "\n",
    "plotly_markers = ['circle', 'square', 'diamond']\n",
    "\n",
    "comps_plots = [1, 2, 3]\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Create a scatter plot for each label\n",
    "traces = []\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    data_for_label = fitted[labels == label]\n",
    "    trace = go.Scatter3d(\n",
    "        x=data_for_label[::1, comps_plots[0]-1],\n",
    "        y=data_for_label[::1, comps_plots[1]-1],\n",
    "        z=data_for_label[::1, comps_plots[2]-1],\n",
    "        mode='markers',\n",
    "        marker=dict(symbol=plotly_markers[idx % len(plotly_markers)], size=2),\n",
    "        name=str(label)\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=f\"comp{comps_plots[0]}\",\n",
    "        yaxis_title=f\"comp{comps_plots[1]}\",\n",
    "        zaxis_title=f\"comp{comps_plots[2]}\"\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    width=800,  # Adjust as needed\n",
    "    height=800   # Adjust as needed\n",
    ")\n",
    "\n",
    "# Create the figure and display it\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c760dfd6",
   "metadata": {},
   "source": [
    "Lets remind ourselves what these components mean\n",
    "\n",
    "* Component 1: Generaly busyness\n",
    "* Component 2: Increase in early morning and late afternoon peaks.\n",
    "* Component 3: Decreases in the midday counts.\n",
    "\n",
    "What do we see here.\n",
    "* As component 1 increases. The spread off *all* points increases. This makes sense as the *busyness* increases so will the height of the morning, midday and evening peaks. This is a crude assessment we can look deeper.\n",
    "* There are a large *wall* of points for which component 1 increases component 2 decreases!. i.e. as the place gets busier and its morning and evening rushes get smaller. This implies that they are experiencing an increase of people and its not work-activity related! This wall of components experiences both positive and negative component three.\n",
    "* There is another group of points who as component 1 increases component 2 increases! i.e as the place gets busier and its morning and evening rushes increases. These places are Southern Cross and Bourke Street. Both transport hubs. They don't experience much variation in component 3 (lunch time rush).\n",
    "* There is another region of points between these for which I can't come up with a good story at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3dbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
